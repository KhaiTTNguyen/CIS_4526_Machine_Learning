{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "The loss functions shall return a scalar, which is the *average* loss of all the examples\n",
    "'''\n",
    "\n",
    "'''\n",
    "For instance, the square loss of all the training examples is computed as below:\n",
    "\n",
    "def squared_loss(train_y, pred_y):\n",
    "\n",
    "    loss = np.mean(np.square(train_y - pred_y))\n",
    "\n",
    "    return loss\n",
    "'''\n",
    "\n",
    "# return average loss --> use np.mean() ???\n",
    "# log(1 + exp(-train_y.pred_y))\n",
    "# train_y = -1/1 vector, pred_y = W.X\n",
    "\n",
    "def logistic_loss(train_y, pred_y):\n",
    "    productVector = np.multiply(train_y, pred_y)\n",
    "    \n",
    "    log_loss = np.log(1 + np.exp(-productVector))\n",
    "    \n",
    "    return np.mean(log_loss)\n",
    "\n",
    "def hinge_loss(train_y, pred_y):\n",
    "    productVector = np.multiply(train_y, pred_y)\n",
    "    hinge_lossVector = np.maximum(0,1-productVector)\n",
    "    return np.mean(hinge_lossVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The regularizers shall compute the loss without considering the bias term in the weights\n",
    "'''\n",
    "def l1_reg(w):\n",
    "    # take sum of absolute values \n",
    "    l1_loss = 0;\n",
    "    for i in range(1,w.size):\n",
    "        l1_loss += abs(w[i])\n",
    "        \n",
    "    return l1_loss\n",
    "\n",
    "def l2_reg(w):\n",
    "    return np.dot(w[1:], np.transpose(w[1:]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_y is a (num_train,) +1/-1 label vector\n",
    "# The complete training process should be implemented within the train_classifier function. \n",
    "# One may expect to perform sufficient number of iterations with gradient descent to update the weights in this function.\n",
    "\n",
    "def train_classifier(train_x, train_y, learn_rate, loss, lambda_val=None, regularizer=None):\n",
    "    \n",
    "    # create pred_y --> goes into loss function\n",
    "    \n",
    "    # build expression for objective function\n",
    "    \n",
    "    # since the Objective function is determined only at run-time\n",
    "    # compute gradient  partial derivative of each and aggregate them in a single gradient vector\n",
    "    \n",
    "    # w = w - learning_rate * deriv(loss function/w) -- for logistic loss only?\n",
    "    \n",
    "    weight_vector = np.random.rand(len(train_x[0]) + 1) # bias term included\n",
    "    num_iters = 10000\n",
    "    h = 0.0001 # numerical_differentiation \n",
    "    for i in range(num_iters):\n",
    "        current_weight = np.copy(weight_vector)\n",
    "        delta_weight = np.zeros(len(train_x[0]) + 1) # produce delta_weight to update weight w = w - delta_weight\n",
    "        \n",
    "        predict_y = test_classifier(current_weight,train_x)\n",
    "        if(lambda_val):\n",
    "            current_loss = loss(train_y, predict_y) + lambda_val*regularizer(current_weight)\n",
    "        else:\n",
    "            current_loss = loss(train_y, predict_y)\n",
    "            \n",
    "        \n",
    "        for index in range(len(delta_weight)):\n",
    "            temp_current_weight = np.copy(current_weight)\n",
    "            temp_current_weight[index] = temp_current_weight[index] + h;\n",
    "                \n",
    "            temp_predict_y = test_classifier(temp_current_weight,train_x)\n",
    "            \n",
    "            # produce loss\n",
    "            if(lambda_val):\n",
    "                temp_loss = loss(train_y, temp_predict_y) + lambda_val*regularizer(temp_current_weight)\n",
    "            else:\n",
    "                temp_loss = loss(train_y, temp_predict_y)\n",
    "            \n",
    "            # partial differentiation\n",
    "            delta_weight[index] = (temp_loss - current_loss) / h\n",
    "\n",
    "        # update weight vector\n",
    "        weight_vector = current_weight - learn_rate*delta_weight    # W = W - dl/dW    \n",
    "           \n",
    "    return weight_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrun pred_y \n",
    "# not a binary (-1/+1) vector \n",
    "# but the inner products of weights and feature values\n",
    "def test_classifier(w, test_x):\n",
    "    pred_y = np.zeros(len(test_x))\n",
    "    for i in range(len(test_x)):\n",
    "        pred_y[i] = np.dot(w[1:], test_x[i]) + w[0]\n",
    "    \n",
    "    return pred_y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dataX,nNumTrainingExamples):\n",
    "    # Standardize the dataset\n",
    "    dataX_train = dataX[:nNumTrainingExamples, :]\n",
    "    dataX_Transposed = dataX_train.transpose()\n",
    "    column = 0\n",
    "    for row in dataX_Transposed:\n",
    "        mean = np.mean(row)\n",
    "        std = np.std(row)\n",
    "        for index in range(len(dataX.transpose()[0])):\n",
    "            dataX[index][column] -= mean\n",
    "            dataX[index][column] /= std\n",
    "        column += 1\n",
    "    return dataX\n",
    "\n",
    "def compute_accuracy(test_y, pred_y):\n",
    "    compute_pred_y = np.copy(pred_y)\n",
    "    for j in range(len(compute_pred_y)):\n",
    "        if(compute_pred_y[j] < 6):\n",
    "            compute_pred_y[j] = -1\n",
    "        elif (compute_pred_y[j] > 6):\n",
    "            compute_pred_y[j] = 1\n",
    "            \n",
    "    # TO-DO: add your code here\n",
    "    comparison = (test_y == compute_pred_y)\n",
    "    match_count = 0\n",
    "\n",
    "    for i in range(len(comparison)):\n",
    "        if (comparison[i] == True):\n",
    "            match_count = match_count + 1 \n",
    "            \n",
    "    return (match_count/len(test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.70238934 -0.97838704 -0.05663289  1.31350977 -0.1550794   0.17150851\n",
      "  0.03084057 -1.28994399  0.28893248  0.38910502  1.46019885]\n",
      "[ 1.86541098e+00 -1.96942968e+00  2.36513097e+00  2.63763775e+00\n",
      "  3.96585250e+00 -1.96942968e+00 -1.57271363e+00  4.46482732e+00\n",
      " -3.10316338e+00 -2.16283604e+00 -1.59392433e+00 -1.57271363e+00\n",
      " -1.62073326e+00 -1.29465783e+00 -3.70743141e+00  6.76137476e-02\n",
      "  5.46352637e-01 -4.24622643e-01  6.76137476e-02 -4.24622643e-01\n",
      " -3.70743141e+00 -4.52904946e-01  8.79313829e-01  1.25517000e-01\n",
      "  6.76137476e-02  5.46352637e-01 -8.92319019e-01  2.03968402e+00\n",
      " -2.08911144e+00 -6.81780549e-01 -1.62067678e+00  1.02032257e+00\n",
      "  1.16774934e+00  2.68168640e+00  2.32980759e+00 -3.17106744e+00\n",
      " -3.17106744e+00 -3.29570374e+00 -4.16271118e+00 -3.29570374e+00\n",
      " -4.16271118e+00  3.66634938e+00 -1.50805050e+00 -1.50805050e+00\n",
      " -1.10998352e+00  5.42574410e+00  3.11388781e+00 -2.55526560e+00\n",
      " -2.55526560e+00  2.59382059e+00 -2.55526560e+00 -3.62269296e+00\n",
      " -3.82346511e+00 -1.58160670e+00 -1.36081363e+00  1.38627846e+00\n",
      "  4.84423037e-01 -2.52143331e+00 -4.42751234e+00  8.25987444e-03\n",
      "  8.25987444e-03  8.89060862e-01 -3.88106809e+00  4.27895687e-01\n",
      "  8.89060862e-01 -3.88106809e+00 -1.35383349e+00  4.89657225e+00\n",
      "  2.94002293e+00  2.94002293e+00  2.53854292e+00 -2.30710638e-03\n",
      " -2.30710638e-03  2.94002293e+00 -2.30710638e-03  4.89657225e+00\n",
      " -1.88280582e+00  4.18344322e+00 -6.02105514e-01  2.53854292e+00\n",
      " -2.92657108e+00 -2.92657108e+00 -1.96677560e+00  1.03642564e+00\n",
      " -2.16892286e+00 -2.16892286e+00  2.41580441e+00 -3.86138617e-01\n",
      " -2.52265412e+00 -4.07062564e+00 -3.18323583e+00 -2.15215883e+00\n",
      "  2.88515048e+00 -1.08444791e+00 -3.18323583e+00 -2.15215883e+00\n",
      " -2.10872668e+00  2.79417581e-01 -2.10872668e+00 -6.31109610e-01\n",
      "  1.23413283e+00 -3.93804393e+00  1.14581626e+00  1.08276416e+00\n",
      " -3.36085920e+00 -2.45274803e-01 -1.88266688e+00 -2.91741289e-02\n",
      " -1.88266688e+00  3.33253231e+00  3.78211160e+00 -5.86951008e-01\n",
      "  1.50748987e+00  1.54890319e+00  3.33253231e+00  3.78211160e+00\n",
      " -3.08580852e+00  2.82635419e-01  7.60811541e-01  7.60811541e-01\n",
      "  1.02158603e+00  7.60811541e-01  3.28024802e+00 -3.04733717e+00\n",
      "  1.65100593e+00 -3.04733717e+00  3.28024802e+00 -2.99042555e+00\n",
      " -3.23807222e+00 -2.99042555e+00 -5.00806652e+00 -3.26557218e+00\n",
      " -3.23807222e+00 -2.73369639e+00  1.10193307e+00 -3.31893347e+00\n",
      " -1.13070157e+00 -1.13070157e+00 -2.36808356e+00 -2.51295414e+00\n",
      " -3.31893347e+00 -4.62702261e+00 -2.12596152e+00 -1.40157143e+00\n",
      " -1.77427433e+00 -1.77427433e+00 -1.77427433e+00 -1.77427433e+00\n",
      " -1.39158124e+00 -1.77427433e+00 -1.40157143e+00 -1.77427433e+00\n",
      " -1.39158124e+00 -3.13532792e-01 -2.39851607e+00 -2.39851607e+00\n",
      " -1.56228409e+00  2.39688148e+00  3.23456412e+00 -6.75832228e-01\n",
      " -9.38186595e-01  2.24447994e+00 -5.34609079e+00 -2.24538372e+00\n",
      " -1.66443030e+00 -1.89239610e+00  1.10567870e+00  3.85507426e+00\n",
      " -2.83993347e+00  6.30785960e-01 -1.72625501e+00 -1.72625501e+00\n",
      "  1.34855677e+00  6.62564873e-01 -6.89001102e-02 -7.08674494e-01\n",
      " -1.02151551e+00  1.00949261e-01  2.69818749e-01 -2.57806252e+00\n",
      "  2.29844886e+00 -3.36410436e-02 -1.15048459e-01 -8.22252960e-01\n",
      "  5.78377089e-01  3.40201140e+00 -2.89608295e+00 -2.89608295e+00\n",
      "  1.91047043e+00 -2.89608295e+00  1.37850391e+00  6.92325135e-01\n",
      "  1.37850391e+00  4.73115764e+00 -4.14496719e+00 -2.54939754e+00\n",
      " -2.09030668e+00 -2.54939754e+00 -2.36349099e+00 -2.10170944e+00\n",
      " -2.10170944e+00 -2.10170944e+00 -2.10170944e+00 -2.10170944e+00\n",
      " -2.10170944e+00 -1.23655606e+00 -1.06844436e+00  1.89592574e+00\n",
      "  2.46110283e+00  4.33565649e+00  3.08151045e+00 -2.10170944e+00\n",
      "  2.70432378e+00 -4.78164303e+00 -2.19643526e-01  1.88117883e+00\n",
      "  1.33895538e-01  1.86947710e+00  3.02992617e+00  1.33895538e-01\n",
      " -7.51113980e-01 -1.90742569e+00 -1.24057321e-01  1.79875569e+00\n",
      " -1.47490949e+00 -3.20203857e+00 -1.92562973e+00  7.54748305e-01\n",
      " -4.07912035e+00 -3.58482374e+00 -2.80201814e+00 -2.80201814e+00\n",
      " -2.80201814e+00 -4.07410711e+00 -2.80201814e+00 -4.07410711e+00\n",
      " -5.45736909e-01  3.75492327e+00  1.64230650e-01  1.18344192e+00\n",
      "  1.10863019e-01 -1.32054728e+00 -1.32054728e+00 -7.42812181e-02\n",
      " -1.32054728e+00  3.48061762e+00  1.81257914e+00 -2.86800005e+00\n",
      " -2.77994801e+00 -7.15906210e-01 -2.10175083e-01 -3.33166885e+00\n",
      " -3.33166885e+00  1.18794367e+00  2.53258007e+00  2.53258007e+00\n",
      "  2.69787411e+00 -3.03165287e+00 -2.25051065e+00  3.98142626e+00\n",
      " -8.68510057e-01 -5.32815979e-01  7.22708712e-01  7.22708712e-01\n",
      " -2.20198652e+00  7.22708712e-01  4.71197060e-01  4.71197060e-01\n",
      "  7.48651283e-01  4.71197060e-01 -3.38470279e+00 -3.24501957e+00\n",
      "  1.07082240e+00  1.54151756e+00  9.30563697e-01  2.32781414e+00\n",
      " -5.46296332e+00  1.70423734e+00  9.30563697e-01  2.32781414e+00\n",
      " -1.57708740e+00  6.43911219e-01 -1.31951033e+00 -3.30566732e+00\n",
      " -1.31951033e+00 -3.13903014e-01 -1.41208587e+00 -1.31951033e+00\n",
      " -3.30566732e+00 -2.03914557e+00 -2.03914557e+00 -2.03914557e+00\n",
      " -2.03914557e+00 -2.03914557e+00 -2.03914557e+00 -2.03914557e+00\n",
      " -2.34548865e-01 -2.03914557e+00  1.43887174e+00 -3.10858642e+00\n",
      " -6.88984269e-01 -9.96641917e-02 -1.15785768e+00 -6.88984269e-01\n",
      " -2.13878284e+00 -1.68380972e+00 -2.13878284e+00 -2.25502373e+00\n",
      " -2.23485505e+00 -2.87040849e+00 -2.77330045e+00 -3.62578358e+00\n",
      " -2.77330045e+00 -1.28965803e+00 -1.28965803e+00  1.45180528e-01\n",
      " -1.03293158e+00  4.69991293e-01 -1.03293158e+00 -5.30348141e-01\n",
      " -6.09793819e-01 -6.09793819e-01 -6.09793819e-01 -5.30348141e-01\n",
      " -6.09793819e-01 -9.95445190e-01 -2.01738417e+00 -2.01738417e+00\n",
      "  2.81644058e+00 -2.01738417e+00  2.81644058e+00 -1.37992084e+00\n",
      "  4.08890118e+00  1.90019838e+00  4.08890118e+00  1.90019838e+00\n",
      " -1.89741452e+00 -1.36088980e+00 -7.20196450e-01 -4.11989591e+00\n",
      " -4.27223180e+00 -2.43298377e-01 -3.55240464e+00 -2.08536966e+00\n",
      " -2.08536966e+00 -3.55240464e+00 -1.41510924e+00 -1.70224425e+00\n",
      " -2.56852256e+00 -1.02753957e+00 -1.02753957e+00 -1.02753957e+00\n",
      "  2.69172151e+00 -3.89995497e-01 -1.37771015e+00 -1.37771015e+00\n",
      " -3.63238801e-03 -2.27763536e-01  2.40692571e-01 -4.28835849e+00\n",
      " -1.78331310e+00 -1.78331310e+00 -5.36691405e+00  4.94927770e+00\n",
      " -1.78331310e+00 -2.33549343e+00 -5.36691405e+00 -3.56599874e+00\n",
      "  6.17793538e-01  3.51032221e+00 -8.21914685e-01 -6.88577904e-01\n",
      "  2.34215855e+00  1.66565144e+00 -2.10149295e+00 -3.74010693e+00\n",
      " -1.48653410e+00 -2.63218645e+00  3.31018861e-02 -5.55947507e-01\n",
      " -1.49687877e+00  6.64144328e-01  3.24091945e-01  3.24091945e-01\n",
      " -7.77236252e-01  3.24091945e-01 -3.70672492e+00  2.56249449e+00\n",
      "  6.58050374e-01  1.58954376e+00  7.05599455e-01  1.71189750e-01\n",
      "  4.30156920e-01  1.37967972e+00  4.30156920e-01  2.98279719e+00\n",
      "  4.14183986e+00  5.46349244e+00  1.67511098e+00  8.49627346e-03\n",
      "  3.84734964e+00  5.20328222e-01  1.48740074e+00  4.05813651e-01\n",
      "  2.36786687e+00 -3.28671010e+00 -1.41667493e+00 -1.58740466e+00\n",
      " -1.03089928e+00 -3.24273686e+00 -2.81298690e-01 -5.75677793e-01\n",
      "  2.94755293e+00 -2.43236614e-01 -6.42877997e+00  2.60961261e-01\n",
      "  1.91183091e+00 -1.67330994e+00  1.20748288e+00  1.01842786e+00\n",
      "  6.50522884e-01 -2.34153015e+00 -2.30346807e+00  2.76349573e+00\n",
      "  4.12290414e+00  8.78090686e-01  1.20212152e+00 -6.15178414e+00\n",
      " -4.41753944e+00 -4.41753944e+00  3.69828842e+00 -2.97892180e+00\n",
      "  2.39873938e+00  1.58585233e+00  1.58585233e+00  1.58585233e+00\n",
      "  1.62391441e+00  1.32052881e-01  7.42090900e-01  8.66786285e-01\n",
      " -3.03502789e+00  8.85817323e-01  7.61121938e-01 -8.78194431e-01\n",
      " -5.67812592e-01 -5.67812592e-01  2.29611634e+00  1.36300605e+00\n",
      "  2.29611634e+00 -5.29750516e-01 -5.15557042e+00 -5.01470963e-01\n",
      " -5.15557042e+00 -5.65177777e-01 -4.14239299e+00 -5.65177777e-01\n",
      "  3.71422633e+00  3.71422633e+00 -5.03035634e-01 -5.03035634e-01\n",
      "  1.65386826e+00  1.29768540e+00  2.04521660e+00  1.29768540e+00\n",
      "  1.76041405e+00 -1.94349619e+00 -2.55446347e-01  2.88811473e+00\n",
      " -2.47045012e+00  1.62953240e+00 -1.31367241e-01  2.87991341e+00\n",
      " -1.28034489e+00  1.62953240e+00  3.09129875e+00  3.75684948e+00\n",
      "  3.75684948e+00 -2.82815311e-01  1.42319841e+00  3.45156374e+00\n",
      "  2.08460479e+00 -2.04894920e+00 -2.50783134e+00  2.07432890e+00\n",
      "  7.93768779e-02 -3.00171801e+00 -1.44550615e+00 -1.35182805e+00\n",
      "  2.58887714e-01  6.95991781e-01  3.53398485e+00  4.56222640e+00\n",
      "  2.58887714e-01 -3.09075926e+00  1.44779257e+00  1.44779257e+00\n",
      " -3.00897006e-01  1.89324630e+00  1.44779257e+00  2.60679914e+00\n",
      "  1.78575797e+00  6.83240934e-01 -1.38484995e+00 -3.67765777e+00\n",
      "  1.66754416e+00 -3.30871875e+00  1.16989975e+00 -3.30871875e+00\n",
      "  1.42654654e+00 -1.40850749e+00 -1.40850749e+00  2.12536099e+00\n",
      " -1.41805890e+00  1.59274368e+00 -3.87543036e+00 -2.76900739e-01\n",
      "  1.56764464e+00  1.60274638e-01 -3.73099059e+00  8.57962910e-01\n",
      " -1.77725610e+00 -1.77725610e+00  1.50960248e+00  1.80752974e+00\n",
      " -2.38399182e+00  3.59727507e+00 -3.48806195e+00  8.75947830e-01\n",
      " -1.50585501e+00  6.90603148e-01 -6.05288066e+00 -5.05141264e+00\n",
      " -9.49047258e-01 -1.57274587e+00  5.13916997e+00  2.68124351e+00\n",
      " -2.59018712e+00 -1.44732355e+00 -1.76234631e+00  2.93553852e+00]\n",
      "0.6425925925925926\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "\n",
    "    # Read the training data file\n",
    "    szDatasetPath = 'winequality-white.csv'\n",
    "    listClasses = []\n",
    "    listAttrs = []\n",
    "    bFirstRow = True\n",
    "    with open(szDatasetPath) as csvFile:\n",
    "        csvReader = csv.reader(csvFile, delimiter=',')\n",
    "        for row in csvReader:\n",
    "            if bFirstRow:\n",
    "                bFirstRow = False\n",
    "                continue\n",
    "            if int(row[-1]) < 6:\n",
    "                listClasses.append(-1)\n",
    "                listAttrs.append(list(map(float, row[1:len(row) - 1])))\n",
    "            elif int(row[-1]) > 6:\n",
    "                listClasses.append(+1)\n",
    "                listAttrs.append(list(map(float, row[1:len(row) - 1])))\n",
    "\n",
    "    dataX = np.array(listAttrs)\n",
    "    dataY = np.array(listClasses)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 5-fold cross-validation\n",
    "\t# Note: in this assignment, preprocessing the feature values will make\n",
    "\t# a big difference on the accuracy. \n",
    "    \n",
    "    nNumTrainingExamples = int((len(dataX)/5)*4)\n",
    "    dataX = normalize(dataX,nNumTrainingExamples)\n",
    "    \n",
    "    trainX = dataX[:nNumTrainingExamples, :]\n",
    "    trainY = dataY[:nNumTrainingExamples]\n",
    "    testX = dataX[nNumTrainingExamples:, :]\n",
    "    testY = dataY[nNumTrainingExamples:]\n",
    "    weight_vector = train_classifier(trainX,trainY,0.1,logistic_loss)\n",
    "    predict_y = test_classifier(weight_vector,testX)\n",
    "    \n",
    "    acc = compute_accuracy(testY,predict_y)\n",
    "    np.set_printoptions(precision=8)\n",
    "    np.set_printoptions(suppress=True)\n",
    "    \n",
    "    print(weight_vector)\n",
    "    print(trainX)\n",
    "    print(predict_y)\n",
    "    print(acc)\n",
    "    print(testY)\n",
    "    \n",
    "    # Perform feature normalization after spliting the data to training and validation set.\n",
    "    # normalize()\n",
    "   # (F - mean)/std_dev \n",
    "    \n",
    "    # The statistics for normalization would be computed on the training set and applied on\n",
    "\t# training and validation set afterwards.\n",
    "    \n",
    "    \n",
    "    # for logistic loss, no regularizer\n",
    "    # Soft-margin SVM - trained with the hinge loss & l2 regularization term\n",
    "    \n",
    "    \n",
    "    # print(weight_vector)\n",
    "    \n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
